{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75b0f9fe",
   "metadata": {},
   "source": [
    "# FedCAD: Federated vs Centralized Learning Analysis\n",
    "\n",
    "This notebook provides comprehensive analysis and visualization of experiments comparing **Centralized** and **Federated Learning** approaches for breast cancer classification.\n",
    "\n",
    "## üìã Notebook Contents:\n",
    "\n",
    "1. **Performance Metrics Comparison** - Side-by-side comparison of accuracy, precision, recall, F1, and AUC-ROC\n",
    "2. **Training Efficiency** - Analysis of training time vs performance trade-offs\n",
    "3. **Training Progress** - Evolution of metrics during training\n",
    "4. **Federated Configuration Trade-offs** - Impact of local epochs vs server rounds on performance and efficiency\n",
    "5. **Detailed Model Analysis** - ROC curves, confusion matrices, and classification reports\n",
    "6. **Summary Statistics** - Key insights and best configurations\n",
    "7. **Export Results** - Save results for reporting\n",
    "\n",
    "## üöÄ Quick Start:\n",
    "\n",
    "1. Run all cells in order\n",
    "2. Results are fetched from W&B or loaded from CSV (fallback mode)\n",
    "3. Visualizations and insights are generated automatically\n",
    "4. Results are exported to `results/` directory\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8e224e",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfb045c",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bc67db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, precision_recall_curve\n",
    "from torchvision.transforms import Compose, Normalize, ToTensor\n",
    "from medmnist import BreastMNIST\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Import your model and utility functions\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from FedCAD.task import Net, load_data, test\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2099e2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Install wandb if not already installed\n",
    "# !pip install wandb -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f258e9f",
   "metadata": {},
   "source": [
    "## 1. WandB Runs Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a74137",
   "metadata": {},
   "source": [
    "**Note on Reproducibility**: This notebook can run in two modes:\n",
    "- **WandB Mode**: Fetches live data from WandB (requires authentication and project access)\n",
    "- **Offline Mode**: Uses exported CSV data (`wandb_runs_export.csv`) for reproducibility\n",
    "\n",
    "The model evaluation section (Section 2) works independently and only requires the saved model file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3f3175",
   "metadata": {},
   "source": [
    "## Comparing Federated vs Centralized Training\n",
    "\n",
    "This notebook compares two training approaches:\n",
    "- **Centralized**: All data trained in one place (baseline)\n",
    "- **Federated**: Data distributed across clients, trained collaboratively\n",
    "\n",
    "### Key Metrics:\n",
    "- **Performance**: accuracy, precision, recall, F1, AUC-ROC\n",
    "- **Efficiency**: training time, convergence speed\n",
    "- **Trade-offs**: communication rounds vs epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1660f537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch runs from WandB API (if authenticated) or load from CSV\n",
    "import wandb\n",
    "\n",
    "try:\n",
    "    api = wandb.Api()\n",
    "    runs = api.runs(\"louisewiljander-ludwig-maximilianuniversity-of-munich/FedCAD\", filters={\"tags\": {\"$nin\": [\"random_search\"]}})\n",
    "    \n",
    "    # Collect run data with all metrics\n",
    "    run_data = []\n",
    "    for run in runs:\n",
    "        training_type = run.config.get('training_type', 'unknown')\n",
    "        \n",
    "        data = {\n",
    "            'run_id': run.id,\n",
    "            'name': run.name,\n",
    "            'training_type': training_type,\n",
    "            'state': run.state,\n",
    "            'lr': run.config.get('lr', None),\n",
    "            'final_test_acc': run.summary.get('final_test_acc', None),\n",
    "            'final_test_loss': run.summary.get('final_test_loss', None),\n",
    "            'final_precision': run.summary.get('final_precision', None),\n",
    "            'final_recall': run.summary.get('final_recall', None),\n",
    "            'final_f1': run.summary.get('final_f1', None),\n",
    "            'final_auc_roc': run.summary.get('final_auc_roc', None),\n",
    "            'total_training_time_min': run.summary.get('total_training_time_min', None),\n",
    "        }\n",
    "        \n",
    "        # Add type-specific metrics\n",
    "        if training_type == 'centralized':\n",
    "            data['epochs'] = run.config.get('epochs', None)\n",
    "            data['total_steps'] = run.config.get('epochs', 0)\n",
    "        elif training_type == 'federated':\n",
    "            data['num_rounds'] = run.config.get('num_rounds', None)\n",
    "            data['local_epochs'] = run.config.get('local_epochs', None)\n",
    "            data['total_local_epochs'] = run.config.get('total_local_epochs', None)\n",
    "            data['total_steps'] = run.config.get('total_local_epochs', 0)\n",
    "        \n",
    "        run_data.append(data)\n",
    "    \n",
    "    df_comparison = pd.DataFrame(run_data)\n",
    "    df_comparison.to_csv('results.csv', index=False)\n",
    "    print(f\"‚úÖ Fetched {len(df_comparison)} runs from WandB (group='comparison')\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not fetch from WandB: {e}\")\n",
    "    print(\"Trying to load from CSV...\")\n",
    "    try:\n",
    "        df_comparison = pd.read_csv('wandb_comparison_export.csv')\n",
    "        print(f\"‚úÖ Loaded {len(df_comparison)} runs from CSV\")\n",
    "    except:\n",
    "        print(\"‚ùå No data available\")\n",
    "        df_comparison = pd.DataFrame()\n",
    "\n",
    "if not df_comparison.empty:\n",
    "    print(f\"\\nData Summary:\")\n",
    "    print(f\"  Centralized runs: {len(df_comparison[df_comparison['training_type']=='centralized'])}\")\n",
    "    print(f\"  Federated runs: {len(df_comparison[df_comparison['training_type']=='federated'])}\")\n",
    "    display(df_comparison[['name', 'training_type', 'final_test_acc', 'final_f1', 'total_training_time_min']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0002f1d4",
   "metadata": {},
   "source": [
    "### 1. Performance Metrics Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e0be90",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_comparison.empty:\n",
    "    # Filter to get the best run of each type\n",
    "    centralized_runs = df_comparison[df_comparison['training_type'] == 'centralized']\n",
    "    federated_runs = df_comparison[df_comparison['training_type'] == 'federated']\n",
    "    \n",
    "    # Create comparison bar chart\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    fig.suptitle('Centralized vs Federated Training Comparison', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    metrics = [\n",
    "        ('final_test_acc', 'Test Accuracy', axes[0, 0]),\n",
    "        ('final_precision', 'Precision', axes[0, 1]),\n",
    "        ('final_recall', 'Recall', axes[0, 2]),\n",
    "        ('final_f1', 'F1 Score', axes[1, 0]),\n",
    "        ('final_auc_roc', 'AUC-ROC', axes[1, 1]),\n",
    "        ('final_test_loss', 'Test Loss', axes[1, 2])\n",
    "    ]\n",
    "    \n",
    "    for metric, label, ax in metrics:\n",
    "        if centralized_runs[metric].notna().any() and federated_runs[metric].notna().any():\n",
    "            central_mean = centralized_runs[metric].mean()\n",
    "            fed_mean = federated_runs[metric].mean()\n",
    "            central_std = centralized_runs[metric].std()\n",
    "            fed_std = federated_runs[metric].std()\n",
    "            \n",
    "            x = ['Centralized', 'Federated']\n",
    "            y = [central_mean, fed_mean]\n",
    "            err = [central_std, fed_std]\n",
    "            \n",
    "            bars = ax.bar(x, y, yerr=err, capsize=5, alpha=0.7, \n",
    "                         color=['#3498db', '#e74c3c'])\n",
    "            ax.set_ylabel(label, fontsize=11)\n",
    "            ax.set_title(label, fontsize=12, fontweight='bold')\n",
    "            ax.grid(axis='y', alpha=0.3)\n",
    "            \n",
    "            # Add value labels on bars\n",
    "            for bar, val in zip(bars, y):\n",
    "                height = bar.get_height()\n",
    "                ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                       f'{val:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print numerical comparison\n",
    "    print(\"\\nüìä Numerical Comparison (Mean ¬± Std):\")\n",
    "    print(\"=\"*70)\n",
    "    for metric, label, _ in metrics:\n",
    "        if centralized_runs[metric].notna().any() and federated_runs[metric].notna().any():\n",
    "            c_mean, c_std = centralized_runs[metric].mean(), centralized_runs[metric].std()\n",
    "            f_mean, f_std = federated_runs[metric].mean(), federated_runs[metric].std()\n",
    "            print(f\"{label:20s} | Central: {c_mean:.3f}¬±{c_std:.3f} | Federated: {f_mean:.3f}¬±{f_std:.3f}\")\n",
    "else:\n",
    "    print(\"No comparison data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e825fa2",
   "metadata": {},
   "source": [
    "### 2. Training Efficiency: Accuracy vs Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c86cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_comparison.empty:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "    \n",
    "    # Plot 1: Accuracy vs Training Time\n",
    "    ax = axes[0]\n",
    "    for training_type, color, marker in [('centralized', '#3498db', 'o'), \n",
    "                                          ('federated', '#e74c3c', 's')]:\n",
    "        data = df_comparison[df_comparison['training_type'] == training_type]\n",
    "        if not data.empty:\n",
    "            ax.scatter(data['total_training_time_min'], data['final_test_acc'],\n",
    "                      s=120, alpha=0.7, color=color, marker=marker, \n",
    "                      label=training_type.capitalize(), edgecolors='black', linewidth=1)\n",
    "    \n",
    "    ax.set_xlabel('Training Time (minutes)', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Test Accuracy', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('Efficiency: Accuracy vs Training Time', fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Training time comparison\n",
    "    ax = axes[1]\n",
    "    if centralized_runs['total_training_time_min'].notna().any() and \\\n",
    "       federated_runs['total_training_time_min'].notna().any():\n",
    "        \n",
    "        c_time = centralized_runs['total_training_time_min'].mean()\n",
    "        f_time = federated_runs['total_training_time_min'].mean()\n",
    "        c_std = centralized_runs['total_training_time_min'].std()\n",
    "        f_std = federated_runs['total_training_time_min'].std()\n",
    "        \n",
    "        bars = ax.bar(['Centralized', 'Federated'], [c_time, f_time],\n",
    "                     yerr=[c_std, f_std], capsize=5, alpha=0.7,\n",
    "                     color=['#3498db', '#e74c3c'])\n",
    "        \n",
    "        ax.set_ylabel('Training Time (minutes)', fontsize=12, fontweight='bold')\n",
    "        ax.set_title('Average Training Time', fontsize=14, fontweight='bold')\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, val, std in zip(bars, [c_time, f_time], [c_std, f_std]):\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{val:.1f}¬±{std:.1f}m', ha='center', va='bottom', fontsize=11)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n‚è±Ô∏è Training Efficiency:\")\n",
    "    print(f\"Centralized: {centralized_runs['total_training_time_min'].mean():.2f} ¬± {centralized_runs['total_training_time_min'].std():.2f} minutes\")\n",
    "    print(f\"Federated: {federated_runs['total_training_time_min'].mean():.2f} ¬± {federated_runs['total_training_time_min'].std():.2f} minutes\")\n",
    "else:\n",
    "    print(\"No timing data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e814e0",
   "metadata": {},
   "source": [
    "### 3. Training Progress Over Time\n",
    "\n",
    "Visualize how test accuracy and loss evolve during training for both approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe64245",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_comparison.empty and len(df_comparison) > 0:\n",
    "    try:\n",
    "        # Fetch training history for best run of each type\n",
    "        api = wandb.Api()\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "        \n",
    "        for training_type, color, ax_idx in [('centralized', '#3498db', 0), \n",
    "                                              ('federated', '#e74c3c', 1)]:\n",
    "            runs_of_type = df_comparison[df_comparison['training_type'] == training_type]\n",
    "            if not runs_of_type.empty:\n",
    "                # Get best run\n",
    "                best_run_id = runs_of_type.nlargest(1, 'final_test_acc').iloc[0]['run_id']\n",
    "                run = api.run(f\"FedCAD/{best_run_id}\")\n",
    "                history = run.history()\n",
    "                \n",
    "                # Plot test accuracy over time\n",
    "                if training_type == 'centralized' and 'test_acc' in history.columns:\n",
    "                    history_clean = history[history['test_acc'].notna()]\n",
    "                    axes[0].plot(history_clean['epoch'], history_clean['test_acc'],\n",
    "                               marker='o', linewidth=2, color=color, \n",
    "                               label=f'{training_type.capitalize()}', markersize=6)\n",
    "                    axes[1].plot(history_clean['epoch'], history_clean['test_loss'],\n",
    "                               marker='o', linewidth=2, color=color,\n",
    "                               label=f'{training_type.capitalize()}', markersize=6)\n",
    "                    \n",
    "                elif training_type == 'federated' and 'test_acc' in history.columns:\n",
    "                    history_clean = history[history['test_acc'].notna()]\n",
    "                    axes[0].plot(history_clean['round'], history_clean['test_acc'],\n",
    "                               marker='s', linewidth=2, color=color,\n",
    "                               label=f'{training_type.capitalize()}', markersize=6)\n",
    "                    axes[1].plot(history_clean['round'], history_clean['test_loss'],\n",
    "                               marker='s', linewidth=2, color=color,\n",
    "                               label=f'{training_type.capitalize()}', markersize=6)\n",
    "        \n",
    "        # Format accuracy plot\n",
    "        axes[0].set_xlabel('Epoch / Round', fontsize=12, fontweight='bold')\n",
    "        axes[0].set_ylabel('Test Accuracy', fontsize=12, fontweight='bold')\n",
    "        axes[0].set_title('Test Accuracy Over Training', fontsize=14, fontweight='bold')\n",
    "        axes[0].legend(fontsize=11)\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Format loss plot\n",
    "        axes[1].set_xlabel('Epoch / Round', fontsize=12, fontweight='bold')\n",
    "        axes[1].set_ylabel('Test Loss', fontsize=12, fontweight='bold')\n",
    "        axes[1].set_title('Test Loss Over Training', fontsize=14, fontweight='bold')\n",
    "        axes[1].legend(fontsize=11)\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Could not fetch training history: {e}\")\n",
    "        print(\"Skipping training progress plots\")\n",
    "else:\n",
    "    print(\"No data available for training progress plots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170b148c",
   "metadata": {},
   "source": [
    "### 4. Federated Learning Configuration Trade-offs\n",
    "\n",
    "Compare different federated learning setups: local epochs vs server rounds, and their impact on performance and efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6299154",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_comparison.empty:\n",
    "    federated_only = df_comparison[df_comparison['training_type'] == 'federated'].copy()\n",
    "    \n",
    "    if not federated_only.empty and len(federated_only) > 1:\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        fig.suptitle('Federated Learning Configuration Analysis', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # Plot 1: Local Epochs vs Accuracy\n",
    "        ax = axes[0, 0]\n",
    "        scatter = ax.scatter(federated_only['local_epochs'], federated_only['final_test_acc'],\n",
    "                           s=federated_only['num_rounds']*50, alpha=0.6, \n",
    "                           c=federated_only['total_training_time_min'], cmap='viridis',\n",
    "                           edgecolors='black', linewidth=1)\n",
    "        ax.set_xlabel('Local Epochs per Round', fontsize=12, fontweight='bold')\n",
    "        ax.set_ylabel('Test Accuracy', fontsize=12, fontweight='bold')\n",
    "        ax.set_title('Local Epochs vs Performance', fontsize=13, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        cbar = plt.colorbar(scatter, ax=ax)\n",
    "        cbar.set_label('Training Time (min)', fontsize=10)\n",
    "        \n",
    "        # Add legend for bubble size\n",
    "        for rounds in sorted(federated_only['num_rounds'].dropna().unique()):\n",
    "            ax.scatter([], [], s=rounds*50, c='gray', alpha=0.6, \n",
    "                      edgecolors='black', linewidth=1,\n",
    "                      label=f'{int(rounds)} rounds')\n",
    "        ax.legend(title='# Rounds', loc='lower right', fontsize=9)\n",
    "        \n",
    "        # Plot 2: Server Rounds vs Accuracy\n",
    "        ax = axes[0, 1]\n",
    "        scatter = ax.scatter(federated_only['num_rounds'], federated_only['final_test_acc'],\n",
    "                           s=federated_only['local_epochs']*50, alpha=0.6,\n",
    "                           c=federated_only['total_training_time_min'], cmap='plasma',\n",
    "                           edgecolors='black', linewidth=1)\n",
    "        ax.set_xlabel('Number of Server Rounds', fontsize=12, fontweight='bold')\n",
    "        ax.set_ylabel('Test Accuracy', fontsize=12, fontweight='bold')\n",
    "        ax.set_title('Server Rounds vs Performance', fontsize=13, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        cbar = plt.colorbar(scatter, ax=ax)\n",
    "        cbar.set_label('Training Time (min)', fontsize=10)\n",
    "        \n",
    "        # Add legend for bubble size\n",
    "        for epochs in sorted(federated_only['local_epochs'].dropna().unique()):\n",
    "            ax.scatter([], [], s=epochs*50, c='gray', alpha=0.6,\n",
    "                      edgecolors='black', linewidth=1,\n",
    "                      label=f'{int(epochs)} local epochs')\n",
    "        ax.legend(title='Local Epochs', loc='lower right', fontsize=9)\n",
    "        \n",
    "        # Plot 3: Total Training Steps (rounds √ó local_epochs) vs Efficiency\n",
    "        ax = axes[1, 0]\n",
    "        federated_only['total_steps'] = federated_only['num_rounds'] * federated_only['local_epochs']\n",
    "        scatter = ax.scatter(federated_only['total_steps'], federated_only['final_test_acc'],\n",
    "                           s=120, alpha=0.7, c=federated_only['total_training_time_min'],\n",
    "                           cmap='coolwarm', edgecolors='black', linewidth=1)\n",
    "        ax.set_xlabel('Total Training Steps (Rounds √ó Local Epochs)', fontsize=12, fontweight='bold')\n",
    "        ax.set_ylabel('Test Accuracy', fontsize=12, fontweight='bold')\n",
    "        ax.set_title('Total Computation vs Performance', fontsize=13, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        cbar = plt.colorbar(scatter, ax=ax)\n",
    "        cbar.set_label('Training Time (min)', fontsize=10)\n",
    "        \n",
    "        # Plot 4: Configuration Efficiency Heatmap\n",
    "        ax = axes[1, 1]\n",
    "        if federated_only['num_rounds'].notna().any() and federated_only['local_epochs'].notna().any():\n",
    "            # Create pivot table for heatmap\n",
    "            pivot_data = federated_only.pivot_table(\n",
    "                values='final_test_acc',\n",
    "                index='local_epochs',\n",
    "                columns='num_rounds',\n",
    "                aggfunc='mean'\n",
    "            )\n",
    "            \n",
    "            if not pivot_data.empty:\n",
    "                sns.heatmap(pivot_data, annot=True, fmt='.3f', cmap='RdYlGn',\n",
    "                           ax=ax, cbar_kws={'label': 'Test Accuracy'},\n",
    "                           linewidths=1, linecolor='white')\n",
    "                ax.set_xlabel('Number of Server Rounds', fontsize=12, fontweight='bold')\n",
    "                ax.set_ylabel('Local Epochs per Round', fontsize=12, fontweight='bold')\n",
    "                ax.set_title('Configuration Performance Heatmap', fontsize=13, fontweight='bold')\n",
    "            else:\n",
    "                ax.text(0.5, 0.5, 'Insufficient data for heatmap', \n",
    "                       ha='center', va='center', transform=ax.transAxes)\n",
    "                ax.set_title('Configuration Performance Heatmap', fontsize=13, fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print configuration analysis\n",
    "        print(\"\\nüîç Federated Configuration Analysis:\")\n",
    "        print(\"=\"*70)\n",
    "        if 'total_steps' in federated_only.columns:\n",
    "            for _, row in federated_only.iterrows():\n",
    "                print(f\"Rounds: {row['num_rounds']:.0f} | Local Epochs: {row['local_epochs']:.0f} | \"\n",
    "                      f\"Total Steps: {row['total_steps']:.0f} | \"\n",
    "                      f\"Accuracy: {row['final_test_acc']:.3f} | \"\n",
    "                      f\"Time: {row['total_training_time_min']:.2f}min\")\n",
    "    else:\n",
    "        print(\"Need at least 2 federated runs with different configurations for comparison\")\n",
    "else:\n",
    "    print(\"No federated data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163baad5",
   "metadata": {},
   "source": [
    "### 5. Model Performance Detailed Analysis\n",
    "\n",
    "ROC curves, confusion matrices, and per-class metrics for the best models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961a3eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and evaluate the best models from each approach\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "_, testloader = load_data(0, 1)\n",
    "\n",
    "def evaluate_model_detailed(model_path, model_name):\n",
    "    \"\"\"Evaluate model and return predictions, probabilities, and labels\"\"\"\n",
    "    try:\n",
    "        model = Net()\n",
    "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        \n",
    "        all_preds, all_probs, all_labels = [], [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in testloader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.squeeze().long()\n",
    "                outputs = model(images)\n",
    "                probs = torch.softmax(outputs, dim=1)\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                \n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_probs.extend(probs.cpu().numpy())\n",
    "                all_labels.extend(labels.numpy())\n",
    "        \n",
    "        return (np.array(all_preds), np.array(all_probs), np.array(all_labels))\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load {model_name}: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "# Evaluate both models\n",
    "centralized_results = evaluate_model_detailed('models/final_model_centralized.pt', 'Centralized')\n",
    "federated_results = evaluate_model_detailed('final_model.pt', 'Federated')\n",
    "\n",
    "# Create comprehensive visualization\n",
    "fig = plt.figure(figsize=(18, 10))\n",
    "gs = fig.add_gridspec(2, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "model_results = [\n",
    "    ('Centralized', centralized_results, '#3498db'),\n",
    "    ('Federated', federated_results, '#e74c3c')\n",
    "]\n",
    "\n",
    "for idx, (name, results, color) in enumerate(model_results):\n",
    "    if results[0] is not None:\n",
    "        preds, probs, labels = results\n",
    "        \n",
    "        # Confusion Matrix\n",
    "        ax_cm = fig.add_subplot(gs[idx, 0])\n",
    "        cm = confusion_matrix(labels, preds)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax_cm,\n",
    "                   cbar_kws={'label': 'Count'})\n",
    "        ax_cm.set_title(f'{name} Model\\nConfusion Matrix', fontsize=12, fontweight='bold')\n",
    "        ax_cm.set_ylabel('True Label', fontsize=10)\n",
    "        ax_cm.set_xlabel('Predicted Label', fontsize=10)\n",
    "        \n",
    "        # ROC Curve\n",
    "        ax_roc = fig.add_subplot(gs[idx, 1])\n",
    "        fpr, tpr, _ = roc_curve(labels, probs[:, 1])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        ax_roc.plot(fpr, tpr, color=color, lw=2, \n",
    "                   label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "        ax_roc.plot([0, 1], [0, 1], 'k--', lw=1, label='Random')\n",
    "        ax_roc.set_xlim([0.0, 1.0])\n",
    "        ax_roc.set_ylim([0.0, 1.05])\n",
    "        ax_roc.set_xlabel('False Positive Rate', fontsize=10)\n",
    "        ax_roc.set_ylabel('True Positive Rate', fontsize=10)\n",
    "        ax_roc.set_title(f'{name} Model\\nROC Curve', fontsize=12, fontweight='bold')\n",
    "        ax_roc.legend(loc=\"lower right\", fontsize=9)\n",
    "        ax_roc.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Precision-Recall Curve\n",
    "        ax_pr = fig.add_subplot(gs[idx, 2])\n",
    "        precision, recall, _ = precision_recall_curve(labels, probs[:, 1])\n",
    "        pr_auc = auc(recall, precision)\n",
    "        ax_pr.plot(recall, precision, color=color, lw=2,\n",
    "                  label=f'PR curve (AUC = {pr_auc:.3f})')\n",
    "        ax_pr.set_xlim([0.0, 1.0])\n",
    "        ax_pr.set_ylim([0.0, 1.05])\n",
    "        ax_pr.set_xlabel('Recall', fontsize=10)\n",
    "        ax_pr.set_ylabel('Precision', fontsize=10)\n",
    "        ax_pr.set_title(f'{name} Model\\nPrecision-Recall Curve', fontsize=12, fontweight='bold')\n",
    "        ax_pr.legend(loc=\"lower left\", fontsize=9)\n",
    "        ax_pr.grid(True, alpha=0.3)\n",
    "\n",
    "fig.suptitle('Detailed Model Performance Comparison', fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.show()\n",
    "\n",
    "# Print classification reports\n",
    "print(\"\\nüìà Detailed Classification Metrics:\\n\")\n",
    "for name, results, _ in model_results:\n",
    "    if results[0] is not None:\n",
    "        preds, _, labels = results\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"{name} Model\")\n",
    "        print(f\"{'='*50}\")\n",
    "        print(classification_report(labels, preds, target_names=['Class 0', 'Class 1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d714ab69",
   "metadata": {},
   "source": [
    "### 6. Summary Statistics & Key Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93240034",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_comparison.empty:\n",
    "    print(\"=\"*80)\n",
    "    print(\" \"*25 + \"EXPERIMENT SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Overall statistics\n",
    "    centralized = df_comparison[df_comparison['training_type'] == 'centralized']\n",
    "    federated = df_comparison[df_comparison['training_type'] == 'federated']\n",
    "    \n",
    "    print(f\"\\nüìä Dataset & Training Overview:\")\n",
    "    print(f\"   Total Experiments: {len(df_comparison)}\")\n",
    "    print(f\"   Centralized Runs: {len(centralized)}\")\n",
    "    print(f\"   Federated Runs: {len(federated)}\")\n",
    "    \n",
    "    # Performance comparison\n",
    "    print(f\"\\nüéØ Performance Comparison:\")\n",
    "    print(f\"   {'Metric':<20} {'Centralized':<20} {'Federated':<20} {'Difference'}\")\n",
    "    print(f\"   {'-'*75}\")\n",
    "    \n",
    "    metrics_to_compare = [\n",
    "        ('final_test_acc', 'Test Accuracy'),\n",
    "        ('final_precision', 'Precision'),\n",
    "        ('final_recall', 'Recall'),\n",
    "        ('final_f1', 'F1 Score'),\n",
    "        ('final_auc_roc', 'AUC-ROC'),\n",
    "    ]\n",
    "    \n",
    "    for metric, label in metrics_to_compare:\n",
    "        if metric in centralized.columns and metric in federated.columns:\n",
    "            c_val = centralized[metric].mean()\n",
    "            f_val = federated[metric].mean()\n",
    "            diff = f_val - c_val\n",
    "            diff_pct = (diff / c_val * 100) if c_val != 0 else 0\n",
    "            \n",
    "            print(f\"   {label:<20} {c_val:.4f} ¬± {centralized[metric].std():.4f}   \"\n",
    "                  f\"{f_val:.4f} ¬± {federated[metric].std():.4f}   \"\n",
    "                  f\"{diff:+.4f} ({diff_pct:+.1f}%)\")\n",
    "    \n",
    "    # Efficiency comparison\n",
    "    print(f\"\\n‚è±Ô∏è  Efficiency Comparison:\")\n",
    "    c_time = centralized['total_training_time_min'].mean()\n",
    "    f_time = federated['total_training_time_min'].mean()\n",
    "    time_diff = f_time - c_time\n",
    "    time_diff_pct = (time_diff / c_time * 100) if c_time != 0 else 0\n",
    "    \n",
    "    print(f\"   Training Time:      {c_time:.2f} ¬± {centralized['total_training_time_min'].std():.2f} min   \"\n",
    "          f\"{f_time:.2f} ¬± {federated['total_training_time_min'].std():.2f} min   \"\n",
    "          f\"{time_diff:+.2f} min ({time_diff_pct:+.1f}%)\")\n",
    "    \n",
    "    # Best configurations\n",
    "    print(f\"\\nüèÜ Best Configurations:\")\n",
    "    \n",
    "    best_centralized = centralized.nlargest(1, 'final_test_acc').iloc[0]\n",
    "    print(f\"\\n   Centralized Best:\")\n",
    "    print(f\"      Name: {best_centralized['name']}\")\n",
    "    print(f\"      Accuracy: {best_centralized['final_test_acc']:.4f}\")\n",
    "    print(f\"      F1 Score: {best_centralized['final_f1']:.4f}\")\n",
    "    print(f\"      Training Time: {best_centralized['total_training_time_min']:.2f} min\")\n",
    "    if 'epochs' in best_centralized:\n",
    "        print(f\"      Epochs: {best_centralized['epochs']:.0f}\")\n",
    "    \n",
    "    if not federated.empty:\n",
    "        best_federated = federated.nlargest(1, 'final_test_acc').iloc[0]\n",
    "        print(f\"\\n   Federated Best:\")\n",
    "        print(f\"      Name: {best_federated['name']}\")\n",
    "        print(f\"      Accuracy: {best_federated['final_test_acc']:.4f}\")\n",
    "        print(f\"      F1 Score: {best_federated['final_f1']:.4f}\")\n",
    "        print(f\"      Training Time: {best_federated['total_training_time_min']:.2f} min\")\n",
    "        if 'num_rounds' in best_federated and 'local_epochs' in best_federated:\n",
    "            print(f\"      Configuration: {best_federated['num_rounds']:.0f} rounds √ó \"\n",
    "                  f\"{best_federated['local_epochs']:.0f} local epochs = \"\n",
    "                  f\"{best_federated['num_rounds'] * best_federated['local_epochs']:.0f} total steps\")\n",
    "    \n",
    "    # Key insights\n",
    "    print(f\"\\nüí° Key Insights:\")\n",
    "    \n",
    "    # Performance gap\n",
    "    acc_gap = federated['final_test_acc'].mean() - centralized['final_test_acc'].mean()\n",
    "    if abs(acc_gap) < 0.01:\n",
    "        print(f\"   ‚Ä¢ Performance is comparable between approaches (Œî = {acc_gap:.4f})\")\n",
    "    elif acc_gap > 0:\n",
    "        print(f\"   ‚Ä¢ Federated learning achieves {acc_gap:.4f} higher accuracy\")\n",
    "    else:\n",
    "        print(f\"   ‚Ä¢ Centralized learning achieves {abs(acc_gap):.4f} higher accuracy\")\n",
    "    \n",
    "    # Efficiency trade-off\n",
    "    if time_diff > 0:\n",
    "        print(f\"   ‚Ä¢ Federated training takes {time_diff:.2f} min longer ({time_diff_pct:.1f}% increase)\")\n",
    "    else:\n",
    "        print(f\"   ‚Ä¢ Federated training is {abs(time_diff):.2f} min faster ({abs(time_diff_pct):.1f}% decrease)\")\n",
    "    \n",
    "    # Federated configuration insights\n",
    "    if not federated.empty and len(federated) > 1:\n",
    "        if 'total_steps' in federated.columns:\n",
    "            corr_steps = federated[['total_steps', 'final_test_acc']].corr().iloc[0, 1]\n",
    "            print(f\"   ‚Ä¢ Correlation between total training steps and accuracy: {corr_steps:.3f}\")\n",
    "        \n",
    "        if 'num_rounds' in federated.columns and 'local_epochs' in federated.columns:\n",
    "            best_config = federated.nlargest(1, 'final_test_acc').iloc[0]\n",
    "            print(f\"   ‚Ä¢ Best federated config: {best_config['num_rounds']:.0f} rounds with \"\n",
    "                  f\"{best_config['local_epochs']:.0f} local epochs\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\\n\")\n",
    "else:\n",
    "    print(\"No data available for summary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a728df5f",
   "metadata": {},
   "source": [
    "### 7. Export Results for Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39612707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export summary results to files for easy reporting\n",
    "if not df_comparison.empty:\n",
    "    # Create results directory\n",
    "    os.makedirs('results', exist_ok=True)\n",
    "    \n",
    "    # Export full comparison data\n",
    "    df_comparison.to_csv('results/full_comparison.csv', index=False)\n",
    "    print(\"‚úÖ Exported full comparison to: results/full_comparison.csv\")\n",
    "    \n",
    "    # Export summary statistics\n",
    "    summary_stats = []\n",
    "    \n",
    "    for training_type in ['centralized', 'federated']:\n",
    "        subset = df_comparison[df_comparison['training_type'] == training_type]\n",
    "        if not subset.empty:\n",
    "            stats = {\n",
    "                'training_type': training_type,\n",
    "                'count': len(subset),\n",
    "                'mean_test_acc': subset['final_test_acc'].mean(),\n",
    "                'std_test_acc': subset['final_test_acc'].std(),\n",
    "                'mean_precision': subset['final_precision'].mean(),\n",
    "                'mean_recall': subset['final_recall'].mean(),\n",
    "                'mean_f1': subset['final_f1'].mean(),\n",
    "                'mean_auc_roc': subset['final_auc_roc'].mean(),\n",
    "                'mean_training_time_min': subset['total_training_time_min'].mean(),\n",
    "                'std_training_time_min': subset['total_training_time_min'].std(),\n",
    "            }\n",
    "            \n",
    "            # Add best run info\n",
    "            best_run = subset.nlargest(1, 'final_test_acc').iloc[0]\n",
    "            stats['best_run_name'] = best_run['name']\n",
    "            stats['best_run_acc'] = best_run['final_test_acc']\n",
    "            stats['best_run_time'] = best_run['total_training_time_min']\n",
    "            \n",
    "            summary_stats.append(stats)\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_stats)\n",
    "    summary_df.to_csv('results/summary_statistics.csv', index=False)\n",
    "    print(\"‚úÖ Exported summary statistics to: results/summary_statistics.csv\")\n",
    "    \n",
    "    # Export best configurations\n",
    "    if not federated.empty:\n",
    "        federated_configs = federated[['name', 'num_rounds', 'local_epochs', 'total_steps',\n",
    "                                       'final_test_acc', 'final_f1', 'total_training_time_min']].copy()\n",
    "        federated_configs = federated_configs.sort_values('final_test_acc', ascending=False)\n",
    "        federated_configs.to_csv('results/federated_configurations.csv', index=False)\n",
    "        print(\"‚úÖ Exported federated configurations to: results/federated_configurations.csv\")\n",
    "    \n",
    "    print(\"\\nüìÅ All results exported to the 'results/' directory\")\n",
    "else:\n",
    "    print(\"No data to export\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
